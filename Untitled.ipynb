{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a3760a4-c5a6-4c39-be3f-a1b53c8a72dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.11/site-packages (4.34.0)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.11/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.local/lib/python3.11/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in ./.local/lib/python3.11/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb17156-549c-402b-b829-d91a8692fc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /packages/envs/tensorflow-gpu-2.10.0/lib/python3.8/site-packages (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53551a33-d567-47b8-8e19-31b9e3afea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27bca365-f0c1-4d1a-8b69-e851eac24617",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7a17f2-4970-4a07-9ba5-cf07c14e978b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0be714161b4f4ba437163ad5241832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = InstructBlipForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c9054a-8afa-4ea4-9e81-45d21cb84423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "url = \"https://raw.githubusercontent.com/salesforce/LAVIS/main/docs/_static/Confusing-Pictures.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a7d9869-ffa8-409c-b1ce-f7de12f0d686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unusual aspect of this image is that a man is ironing clothes on the back of a yellow SUV, which is parked in the middle of a busy city street. This is an unconventional approach to ironing clothes, as it requires the man to balance himself and his ironing equipment on top of the vehicle while navigating through traffic. Additionally, the presence of taxis and other vehicles in the scene further emphasizes the unusual nature of this situation.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ab401c5-0cda-4ba3-ac57-81d5362ec35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file1 = open(\"/scratch/nmachav1/CLEVR_v1.0/datasetSplits/val_images_objectsNum9.txt\", \"r\")\n",
    "imagesPath = \"/scratch/nmachav1/CLEVR_v1.0/images/val/\"\n",
    "quesPath = \"/scratch/nmachav1/CLEVR_v1.0/questions/CLEVR_val_questions.json\"\n",
    "\n",
    "jsonFile = open(quesPath, 'r')\n",
    "questions  = json.load(jsonFile)\n",
    "\n",
    "Lines = file1.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c7657f4-38ff-4b4c-bf5a-f91b703610bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [20:52<00:00, 12.52s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "ansFile = open(\"/scratch/nmachav1/CLEVR_v1.0/model_answers/val/Num9.txt\", \"w\")\n",
    "for line in tqdm(Lines[:100]):\n",
    "    image_ques = []\n",
    "    image_ans = []\n",
    "    count = 0\n",
    "    img = Image.open(imagesPath+line[:-1]).convert(\"RGB\")\n",
    "    for q in questions[\"questions\"]:\n",
    "        if q[\"image_filename\"] == line[:-1]:\n",
    "            image_ques.append(q[\"question\"])\n",
    "            image_ans.append(q[\"answer\"])\n",
    "    for iqs in image_ques:\n",
    "        prompt=iqs\n",
    "        inputs = processor(images=img, text=prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            num_beams=5,\n",
    "            max_length=256,\n",
    "            min_length=1,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.5,\n",
    "            length_penalty=1.0,\n",
    "            temperature=1,\n",
    "        )\n",
    "        generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "        ansFile.write(prompt +\" \" + image_ans[count] + \": \" + generated_text+\"\\n\")\n",
    "        count+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04347063-fb25-4e1d-9ead-d3a1740ec488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file1 = open(\"/scratch/nmachav1/CLEVR_v1.0/datasetSplits/val_images_objectsNum3.txt\", \"r\")\n",
    "imagesPath = \"/scratch/nmachav1/CLEVR_v1.0/images/val/\"\n",
    "quesPath = \"/scratch/nmachav1/CLEVR_v1.0/questions/CLEVR_val_questions.json\"\n",
    "\n",
    "jsonFile = open(quesPath, 'r')\n",
    "questions  = json.load(jsonFile)\n",
    "\n",
    "Lines = file1.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd5f9816-df4b-49f3-872b-788882998c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [23:04<00:00, 13.84s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "ansFile = open(\"/scratch/nmachav1/CLEVR_v1.0/model_answers/val/Num3.txt\", \"a\")\n",
    "for line in tqdm(Lines[:100]):\n",
    "    image_ques = []\n",
    "    image_ans = []\n",
    "    count = 0\n",
    "    img = Image.open(imagesPath+line[:-1]).convert(\"RGB\")\n",
    "    for q in questions[\"questions\"]:\n",
    "        if q[\"image_filename\"] == line[:-1]:\n",
    "            image_ques.append(q[\"question\"])\n",
    "            image_ans.append(q[\"answer\"])\n",
    "    for iqs in image_ques:\n",
    "        prompt=iqs\n",
    "        inputs = processor(images=img, text=prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            num_beams=5,\n",
    "            max_length=256,\n",
    "            min_length=1,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.5,\n",
    "            length_penalty=1.0,\n",
    "            temperature=1,\n",
    "        )\n",
    "        generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "        ansFile.write(prompt +\" \" + image_ans[count] + \": \" + generated_text+\"\\n\")\n",
    "        count+=1\n",
    "ansFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf94cab6-c6cd-45fe-8e59-9940b7eeffca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.0.1",
   "language": "python",
   "name": "pytorch-gpu-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
