{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b21fdbfb-850e-4d87-9b05-73eb93b6d237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: open_flamingo in /home/nmachav1/.local/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: einops in /home/nmachav1/.local/lib/python3.11/site-packages (from open_flamingo) (0.7.0)\n",
      "Requirement already satisfied: einops-exts in /home/nmachav1/.local/lib/python3.11/site-packages (from open_flamingo) (0.0.4)\n",
      "Requirement already satisfied: transformers>=4.28.1 in /home/nmachav1/.local/lib/python3.11/site-packages (from open_flamingo) (4.34.0)\n",
      "Requirement already satisfied: torch==2.0.1 in /home/nmachav1/.local/lib/python3.11/site-packages (from open_flamingo) (2.0.1)\n",
      "Requirement already satisfied: pillow in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from open_flamingo) (9.4.0)\n",
      "Requirement already satisfied: open-clip-torch>=2.16.0 in /home/nmachav1/.local/lib/python3.11/site-packages (from open_flamingo) (2.23.0)\n",
      "Requirement already satisfied: sentencepiece==0.1.98 in /home/nmachav1/.local/lib/python3.11/site-packages (from open_flamingo) (0.1.98)\n",
      "Requirement already satisfied: filelock in /home/nmachav1/.local/lib/python3.11/site-packages (from torch==2.0.1->open_flamingo) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from torch==2.0.1->open_flamingo) (4.8.0)\n",
      "Requirement already satisfied: sympy in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from torch==2.0.1->open_flamingo) (1.12)\n",
      "Requirement already satisfied: networkx in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from torch==2.0.1->open_flamingo) (3.1)\n",
      "Requirement already satisfied: jinja2 in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from torch==2.0.1->open_flamingo) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/nmachav1/.local/lib/python3.11/site-packages (from torch==2.0.1->open_flamingo) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/nmachav1/.local/lib/python3.11/site-packages (from torch==2.0.1->open_flamingo) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/nmachav1/.local/lib/python3.11/site-packages (from torch==2.0.1->open_flamingo) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/nmachav1/.local/lib/python3.11/site-packages (from torch==2.0.1->open_flamingo) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/nmachav1/.local/lib/python3.11/site-packages (from torch==2.0.1->open_flamingo) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/nmachav1/.local/lib/python3.11/site-packages (from torch==2.0.1->open_flamingo) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/nmachav1/.local/lib/python3.11/site-packages (from torch==2.0.1->open_flamingo) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/nmachav1/.local/lib/python3.11/site-packages (from torch==2.0.1->open_flamingo) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/nmachav1/.local/lib/python3.11/site-packages (from torch==2.0.1->open_flamingo) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/nmachav1/.local/lib/python3.11/site-packages (from torch==2.0.1->open_flamingo) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/nmachav1/.local/lib/python3.11/site-packages (from torch==2.0.1->open_flamingo) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/nmachav1/.local/lib/python3.11/site-packages (from torch==2.0.1->open_flamingo) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->open_flamingo) (68.2.2)\n",
      "Requirement already satisfied: wheel in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->open_flamingo) (0.41.2)\n",
      "Requirement already satisfied: cmake in /home/nmachav1/.local/lib/python3.11/site-packages (from triton==2.0.0->torch==2.0.1->open_flamingo) (3.27.7)\n",
      "Requirement already satisfied: lit in /home/nmachav1/.local/lib/python3.11/site-packages (from triton==2.0.0->torch==2.0.1->open_flamingo) (17.0.5)\n",
      "Requirement already satisfied: torchvision in /home/nmachav1/.local/lib/python3.11/site-packages (from open-clip-torch>=2.16.0->open_flamingo) (0.15.2)\n",
      "Requirement already satisfied: regex in /home/nmachav1/.local/lib/python3.11/site-packages (from open-clip-torch>=2.16.0->open_flamingo) (2023.10.3)\n",
      "Requirement already satisfied: ftfy in /home/nmachav1/.local/lib/python3.11/site-packages (from open-clip-torch>=2.16.0->open_flamingo) (6.1.1)\n",
      "Requirement already satisfied: tqdm in /home/nmachav1/.local/lib/python3.11/site-packages (from open-clip-torch>=2.16.0->open_flamingo) (4.66.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/nmachav1/.local/lib/python3.11/site-packages (from open-clip-torch>=2.16.0->open_flamingo) (0.17.3)\n",
      "Requirement already satisfied: protobuf in /home/nmachav1/.local/lib/python3.11/site-packages (from open-clip-torch>=2.16.0->open_flamingo) (4.25.0)\n",
      "Requirement already satisfied: timm in /home/nmachav1/.local/lib/python3.11/site-packages (from open-clip-torch>=2.16.0->open_flamingo) (0.9.10)\n",
      "Requirement already satisfied: numpy>=1.17 in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from transformers>=4.28.1->open_flamingo) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from transformers>=4.28.1->open_flamingo) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from transformers>=4.28.1->open_flamingo) (6.0.1)\n",
      "Requirement already satisfied: requests in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from transformers>=4.28.1->open_flamingo) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/nmachav1/.local/lib/python3.11/site-packages (from transformers>=4.28.1->open_flamingo) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/nmachav1/.local/lib/python3.11/site-packages (from transformers>=4.28.1->open_flamingo) (0.4.0)\n",
      "Requirement already satisfied: fsspec in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from huggingface-hub->open-clip-torch>=2.16.0->open_flamingo) (2023.9.2)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from ftfy->open-clip-torch>=2.16.0->open_flamingo) (0.2.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from jinja2->torch==2.0.1->open_flamingo) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from requests->transformers>=4.28.1->open_flamingo) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from requests->transformers>=4.28.1->open_flamingo) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from requests->transformers>=4.28.1->open_flamingo) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from requests->transformers>=4.28.1->open_flamingo) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /packages/envs/pytorch-gpu-2.0.1/lib/python3.11/site-packages (from sympy->torch==2.0.1->open_flamingo) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install open_flamingo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89298932-8236-4128-becb-5dce8aa35881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3658557c764f71946588256262b987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flamingo model initialized with 1384781840 trainable parameters\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "\n",
    "model, image_processor, tokenizer = create_model_and_transforms(\n",
    "    clip_vision_encoder_path=\"ViT-L-14\",\n",
    "    clip_vision_encoder_pretrained=\"openai\",\n",
    "    lang_encoder_path=\"anas-awadalla/mpt-7b\",\n",
    "    tokenizer_path=\"anas-awadalla/mpt-7b\",\n",
    "    cross_attn_every_n_layers=4\n",
    ")\n",
    "\n",
    "# grab model checkpoint from huggingface hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-9B-vitl-mpt7b\", \"checkpoint.pt\")\n",
    "model.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76308c1b-acd4-45ac-984c-b9866cccdbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "imagesPath = \"/scratch/nmachav1/CLEVR_v1.0/images/val/\"\n",
    "quesPath = \"/scratch/nmachav1/CLEVR_v1.0/questions/CLEVR_val_questions.json\"\n",
    "\n",
    "jsonFile = open(quesPath, 'r')\n",
    "questions  = json.load(jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aa8bb8d-b6d3-4bc9-9fda-18a18a894a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Question: What is the material of the thing that is left of the blue block and on the right side of the big green matte block? Answer:  The material of the thing that is left of the blue block and on the right side of the big green matte block is the material of the big green matte block.<|endofchunk|>\n",
      "The material of the thing that is left of the blue block and on the right side of the big green matte block is the material of the big green matte block.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Question: Is the shape of the small gray matte thing the same as the object behind the big green rubber object? Answer:  Yes.<|endofchunk|>\n",
      "Yes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Question: What is the big thing that is in front of the block that is behind the block that is in front of the large shiny block made of? Answer:  The big thing that is in front of the block that is behind the block that is in front of the large shiny block made of.<|endofchunk|>\n",
      "The big thing that is in front of the block that is behind the block that is in front of the large shiny block made of.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Question: How many other objects are the same size as the green rubber object? Answer:  3<|endofchunk|>\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Question: Is the color of the large shiny object the same as the small ball? Answer:  Yes.<|endofchunk|>\n",
      "Yes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Question: There is a cube to the left of the rubber thing that is on the right side of the large green matte block; how many big blue metallic objects are right of it? Answer:  2.<|endofchunk|>\n",
      "2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Question: There is a large block that is behind the block to the left of the big blue thing; what is it made of? Answer:  It is made of the same material as the big blue thing.<|endofchunk|>\n",
      "It is made of the same material as the big blue thing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Question: Is there a small gray thing that has the same shape as the big green object? Answer:  Yes.<|endofchunk|>\n",
      "Yes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Question: What is the color of the metal object that is the same size as the green rubber block? Answer:  Blue<|endofchunk|>\n",
      "Blue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:21<00:00, 81.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Question: What number of things are objects behind the big green matte cube or things that are in front of the big shiny thing? Answer:  1.<|endofchunk|>\n",
      "1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "for i in range(1,4):\n",
    "    img_file = open(f\"/scratch/nmachav1/CLEVR_v1.0/datasetSplits/val_images_objectsNum{i}.txt\", \"r\") \n",
    "    img_names = img_file.readlines()\n",
    "    \n",
    "    ansFile = open(f\"/scratch/nmachav1/CLEVR_v1.0/OpenFlamingo/answers_difficulty_based_val/num_{i}.txt\", \"a\")\n",
    "    object = []\n",
    "    for img_name in tqdm(img_names[:1]):\n",
    "        image_ques = []\n",
    "        image_ans = []\n",
    "        count = 0\n",
    "        img = Image.open(imagesPath+img_name[:-1]).convert(\"RGB\")\n",
    "        vision_x = [image_processor(img).unsqueeze(0)]\n",
    "        vision_x = torch.cat(vision_x, dim=0).to(device)\n",
    "        vision_x = vision_x.unsqueeze(1).unsqueeze(0)\n",
    "\n",
    "        for q in questions[\"questions\"]:\n",
    "            if q[\"image_filename\"] == img_name[:-1]:\n",
    "                image_ques.append(q[\"question\"])\n",
    "                image_ans.append(q[\"answer\"])\n",
    "        for iqs in image_ques:\n",
    "            ques_for_model = \"Question: \" + iqs + \" Answer: \"\n",
    "            tokenizer.padding_side = \"left\"  # For generation, padding tokens should be on the left\n",
    "            lang_x = tokenizer(\n",
    "                [\"<image>\" + ques_for_model],\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "\n",
    "            generated_text = model.generate(\n",
    "                vision_x=vision_x,\n",
    "                lang_x=lang_x[\"input_ids\"],\n",
    "                attention_mask=lang_x[\"attention_mask\"],\n",
    "                max_new_tokens=50,  # You can adjust this to control the length of the generated text\n",
    "                num_beams=6,\n",
    "            )\n",
    "\n",
    "            model_output = tokenizer.decode(generated_text[0])\n",
    "            print(model_output)\n",
    "            answer_index = model_output.split(\"Answer:\")\n",
    "\n",
    "            if \"<|\" in answer_index[-1]:\n",
    "                answer = answer_index[-1].split(\"<|\")[0].strip()\n",
    "            else:\n",
    "                answer = answer_index[-1].strip()\n",
    "            print(answer)\n",
    "            object.append({'image_id': img_name,\n",
    "                          'Question': iqs,\n",
    "                          'Ground truth': image_ans[count],\n",
    "                          'Model generated answer': answer\n",
    "            })\n",
    "\n",
    "            count+=1\n",
    "    json.dump(object, ansFile)\n",
    "    ansFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66cf00e5-acfa-4121-b993-05f9650e37c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Question: What is the big thing that is in front of the block that is behind the block that is in front of the large shiny block made of? Answer:  The big thing that is in front of the block that is behind the block that is in front of the large shiny block made of.<|endofchunk|>\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "\"\"\"\n",
    "Step 1: Load images\n",
    "val_images_objectsNum1.txt\n",
    "\"\"\"\n",
    "demo_image_one = Image.open(\"/scratch/nmachav1/CLEVR_v1.0/images/val/CLEVR_val_000006.png\").convert(\"RGB\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 2: Preprocessing images\n",
    "Details: For OpenFlamingo, we expect the image to be a torch tensor of shape \n",
    " batch_size x num_media x num_frames x channels x height x width. \n",
    " In this case batch_size = 1, num_media = 3, num_frames = 1,\n",
    " channels = 3, height = 224, width = 224.\n",
    "\"\"\"\n",
    "vision_x = [image_processor(demo_image_one).unsqueeze(0)]\n",
    "vision_x = torch.cat(vision_x, dim=0).to(device)\n",
    "vision_x = vision_x.unsqueeze(1).unsqueeze(0)\n",
    "\n",
    "\"\"\"\n",
    "Step 3: Preprocessing text\n",
    "Details: In the text we expect an <image> special token to indicate where an image is.\n",
    " We also expect an <|endofchunk|> special token to indicate the end of the text \n",
    " portion associated with an image.\n",
    "\"\"\"\n",
    "question = \"Question: What is the big thing that is in front of the block that is behind the block that is in front of the large shiny block made of? Answer: \"\n",
    "tokenizer.padding_side = \"left\"  # For generation, padding tokens should be on the left\n",
    "lang_x = tokenizer(\n",
    "    [\"<image>\" + question],\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Step 4: Generate text\n",
    "\"\"\"\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x,\n",
    "    lang_x=lang_x[\"input_ids\"],\n",
    "    attention_mask=lang_x[\"attention_mask\"],\n",
    "    max_new_tokens=50,  # You can adjust this to control the length of the generated text\n",
    "    num_beams=6,\n",
    ")\n",
    "model_output = tokenizer.decode(generated_text[0])\n",
    "print(model_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a2c32d6-5869-4314-ba9a-f33d9aa43a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The material of the thing that is left of the blue block and on the right side of the big green matte block is the material of the big green matte block.\n"
     ]
    }
   ],
   "source": [
    "answer_index = model_output.split(\"Answer:\")\n",
    "if \"<|\" in answer_index[-1]:\n",
    "    answer = answer_index[-1].split(\"<|\")[0].strip()\n",
    "else:\n",
    "    answer = answer_index[-1].strip()\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d51afca5-fe89-4b88-b58a-1acc203bbbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ansFile = open(f\"/scratch/nmachav1/CLEVR_v1.0/OpenFlamingo/answers_difficulty_based_val/num_{i}.txt\", \"a+\")\n",
    "ansFile.seek(0)\n",
    "\n",
    "# Check if the file is empty\n",
    "is_empty = not bool(ansFile.read())\n",
    "\n",
    "# If the file is not empty, truncate it to remove existing content\n",
    "if not is_empty:\n",
    "    ansFile.truncate(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9012164-b089-46f3-98b8-66fa3c0fc2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.0.1",
   "language": "python",
   "name": "pytorch-gpu-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
