{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from PIL import Image\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053101ef117a4cab803fbe79610d631d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "model = InstructBlipForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n",
    "\n",
    "\n",
    "processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = \"/scratch/averma90/CLEVR_v1.0/images/val/\"\n",
    "QUES_PATH = \"/home/averma90/CSE576/github/CLEVR_v1.0/questions/CLEVR_val_questions.json\"\n",
    "\n",
    "json_file = open(QUES_PATH, 'r')\n",
    "questions  = json.load(json_file)['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,4):\n",
    "    img_file = open(f\"/home/averma90/CSE576/github/MLLM_Hallucinations/CLEVR_v1/datasetSplits/val_images_objectsNum{i}.txt\", \"r\") \n",
    "    img_names = img_file.readlines()\n",
    "    \n",
    "    ansFile = open(f\"/home/averma90/CSE576/github/CLEVR_v1.0/answers/val/json_num_{i}.json\", \"a\")\n",
    "    object = []\n",
    "\n",
    "    for img_name in tqdm(img_names[:100])\n",
    "        image_ques = []\n",
    "        image_ans = []\n",
    "        count = 0\n",
    "        img = Image.open(IMAGES_PATH+img_name[:-1]).convert(\"RGB\")\n",
    "        for q in questions:\n",
    "            if q[\"image_filename\"] == img_name[:-1]:\n",
    "                image_ques.append(q[\"question\"])\n",
    "                image_ans.append(q[\"answer\"])\n",
    "        for iqs in image_ques:\n",
    "            prompt=iqs\n",
    "            inputs = processor(images=img, text=prompt, return_tensors=\"pt\").to(device)\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=False,\n",
    "                num_beams=5,\n",
    "                max_length=256,\n",
    "                min_length=1,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.5,\n",
    "                length_penalty=1.0,\n",
    "                temperature=1,\n",
    "            )\n",
    "            generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "            # print(generated_text)\n",
    "            object.append({'image_id': img_name,\n",
    "                          'Question': prompt,\n",
    "                          'Ground truth': image_ans[count],\n",
    "                          'Model generated answer': generated_text\n",
    "            })\n",
    "            # ansFile.write(f\"{prompt} Answer: {image_ans[count]}. Output: {generated_text}\\n\")\n",
    "            \n",
    "            # ansFile.write(prompt +\" \" + image_ans[count] + \": \" + generated_text+\"\\n\")\n",
    "            count+=1\n",
    "\n",
    "    json.dump(object, ansFile)\n",
    "    ansFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareAllQuestionsAnswers(all_questions, image_file_name_list):\n",
    "    print(len(all_questions))\n",
    "\n",
    "    questions = []\n",
    "    answers = []\n",
    "    images = []\n",
    "    \n",
    "    for img_name in tqdm(image_file_name_list[:1]):                                                       ######## CHANGE THIS ###########\n",
    "        for q in all_questions:\n",
    "            if q[\"image_filename\"] == img_name[:-1]:\n",
    "                questions.append(q[\"question\"])\n",
    "                answers.append(q[\"answer\"])\n",
    "                images.append(q[\"image_filename\"])\n",
    "\n",
    "    return images, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDataset(image_list_file, questions):\n",
    "    image_file_name_list = open(image_list_file, \"r\").readlines()\n",
    "    input_images_array, input_questions_array, input_answers_array = prepareAllQuestionsAnswers(questions, image_file_name_list)\n",
    "    \n",
    "    print(len(input_images_array), len(input_questions_array), len(input_answers_array))\n",
    "    \n",
    "    return input_images_array, input_questions_array, input_answers_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference():\n",
    "    batch_size = 1\n",
    "    # for i in range(1,11):\n",
    "    \n",
    "    \n",
    "    for i in range(1,4):\n",
    "        answers = []\n",
    "        image_array = []\n",
    "        imgs_file = f\"/home/averma90/CSE576/github/MLLM_Hallucinations/CLEVR_v1/datasetSplits/val_images_objectsNum{i}.txt\"\n",
    "        save_file = f\"/home/averma90/CSE576/github/MLLM_Hallucinations/CLEVR_v1/answers/val/json_answers_{i}.json\"\n",
    "\n",
    "        answerObj = []\n",
    "        \n",
    "        input_images_array, input_questions_array, input_answers_array = prepareDataset(imgs_file, questions)\n",
    "\n",
    "        for img_name in tqdm(input_images_array):                                                      \n",
    "            image_array.append(Image.open(IMAGES_PATH+img_name).convert(\"RGB\"))\n",
    "            \n",
    "        image_batches = [image_array[i:i+batch_size] for i in range(0, len(input_images_array), batch_size)]\n",
    "        questions_batches = [input_questions_array[i:i+batch_size] for i in range(0, len(input_images_array), batch_size)]\n",
    "        answers_batches = [input_answers_array[i:i+batch_size] for i in range(0, len(input_images_array), batch_size)]\n",
    "\n",
    "        for index in tqdm(range(len(image_batches))):\n",
    "            print(image_batches[index], questions_batches[index], answers_batches[index])\n",
    "            # print(image_batches)\n",
    "            inputs = processor(images=image_batches[index], text=questions_batches[index], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            print(len(inputs))\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=False,\n",
    "                use_cache= True,\n",
    "                num_beams=5,\n",
    "                max_length=256,\n",
    "                min_length=None,\n",
    "                top_p=1.0,\n",
    "                top_k= 50,\n",
    "                repetition_penalty=1.0,\n",
    "                length_penalty=1.0,\n",
    "                max_padding_length= None,\n",
    "                temperature=1e-05,\n",
    "            )\n",
    "\n",
    "            generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "            \n",
    "\n",
    "        with open(save_file) as f:\n",
    "            json.dump(answerObj, f)\n",
    "            \n",
    "        # print(answers)\n",
    "    # return answers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 32.35it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<PIL.Image.Image image mode=RGB size=480x320 at 0x154D0C57C990>, <PIL.Image.Image image mode=RGB size=480x320 at 0x154D0C57A4D0>] ['What is the material of the thing that is left of the blue block and on the right side of the big green matte block?', 'Is the shape of the small gray matte thing the same as the object behind the big green rubber object?'] ['rubber', 'no']\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/averma90/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1e-05` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/5 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,  715, 6288,    2,    1],\n",
      "        [   2,  694,    2,    1,   -1]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "out of range integral type conversion attempted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 42\u001b[0m, in \u001b[0;36minference\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m     28\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-05\u001b[39m,\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/instructblip/processing_instructblip.py:140\u001b[0m, in \u001b[0;36mInstructBlipProcessor.batch_decode\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    refer to the docstring of this method for more information.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3698\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3675\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3676\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3679\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3680\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3681\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3682\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3683\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3696\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3697\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[1;32m   3699\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3700\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3703\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3704\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3705\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msequences\u001b[49m\n\u001b[1;32m   3706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3699\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3675\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3676\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3679\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3680\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3681\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3682\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3683\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3696\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3697\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m-> 3699\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3700\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3703\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3704\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3705\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3706\u001b[0m     ]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3738\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3735\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3736\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3738\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3742\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3743\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:625\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    624\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 625\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    628\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    631\u001b[0m )\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mOverflowError\u001b[0m: out of range integral type conversion attempted"
     ]
    }
   ],
   "source": [
    "inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iqs in image_ques[:1]:\n",
    "    prompt=iqs\n",
    "    inputs = processor(images=[img, img], text=[prompt, prompt], return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        use_cache= True,\n",
    "        num_beams=5,\n",
    "        max\\length=256,\n",
    "        min_length=None,\n",
    "        top_p=1.0,\n",
    "        top_k= 50,\n",
    "        repetition_penalty=1.0,\n",
    "        length_penalty=1.0,\n",
    "        max_padding_length= None,\n",
    "        temperature=1e-05,\n",
    "    )\n",
    "    generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "    # print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plastic', 'plastic']\n"
     ]
    }
   ],
   "source": [
    "print(processor.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 100/100 [23:19<00:00, 14.00s/it]\n",
      "100%|██████████| 100/100 [20:38<00:00, 12.38s/it]\n",
      "100%|██████████| 100/100 [20:35<00:00, 12.35s/it]\n",
      "100%|██████████| 100/100 [21:10<00:00, 12.71s/it]\n",
      "100%|██████████| 100/100 [20:21<00:00, 12.22s/it]\n",
      "100%|██████████| 100/100 [18:04<00:00, 10.84s/it]\n",
      "100%|██████████| 100/100 [21:03<00:00, 12.64s/it]\n",
      "100%|██████████| 100/100 [19:55<00:00, 11.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# for i in range(1,11):\n",
    "    img_file = open(f\"/home/averma90/CSE576/github/MLLM_Hallucinations/CLEVR_v1/datasetSplits/val_images_objectsNum{i}.txt\", \"r\") \n",
    "    img_names = img_file.readlines()\n",
    "    \n",
    "    ansFile = open(f\"/home/averma90/CSE576/github/CLEVR_v1.0/answers/val/num_{i}.txt\", \"a\")\n",
    "    \n",
    "    for img_name in tqdm(img_names[:100]):\n",
    "        image_ques = []\n",
    "        image_ans = []\n",
    "        count = 0\n",
    "        img = Image.open(imagesPath+img_name[:-1]).convert(\"RGB\")\n",
    "        for q in questions[\"questions\"]:\n",
    "            if q[\"image_filename\"] == img_name[:-1]:\n",
    "                image_ques.append(q[\"question\"])\n",
    "                image_ans.append(q[\"answer\"])\n",
    "        for iqs in image_ques:\n",
    "            prompt=iqs\n",
    "            inputs = processor(images=img, text=prompt, return_tensors=\"pt\").to(device)\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=False,\n",
    "                num_beams=5,\n",
    "                max_length=256,\n",
    "                min_length=1,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.5,\n",
    "                length_penalty=1.0,\n",
    "                temperature=1,\n",
    "            )\n",
    "            generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "            # print(generated_text)\n",
    "            ansFile.write(f\"{prompt} Answer: {image_ans[count]}. Output: {generated_text}\\n\")\n",
    "            # ansFile.write(prompt +\" \" + image_ans[count] + \": \" + generated_text+\"\\n\")\n",
    "            count+=1\n",
    "    ansFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/315 [00:00<?, ?it/s]/home/averma90/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 315/315 [1:04:35<00:00, 12.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# for objects_type in ['inter', 'intra']:\n",
    "for objects_type in ['intra']:\n",
    "    img_file = open(f\"/home/averma90/CSE576/github/MLLM_Hallucinations/CLEVR_v1/datasetSplits/val_images_{objects_type}.txt\", \"r\") \n",
    "    img_names = img_file.readlines()\n",
    "    \n",
    "    ansFile = open(f\"/home/averma90/CSE576/github/CLEVR_v1.0/answers/val/type_{objects_type}.txt\", \"a\")\n",
    "    \n",
    "    for img_name in tqdm(img_names[:315]):\n",
    "        image_ques = []\n",
    "        image_ans = []\n",
    "        count = 0\n",
    "        img = Image.open(imagesPath+img_name[:-1]).convert(\"RGB\")\n",
    "        for q in questions[\"questions\"]:\n",
    "            if q[\"image_filename\"] == img_name[:-1]:\n",
    "                image_ques.append(q[\"question\"])\n",
    "                image_ans.append(q[\"answer\"])\n",
    "        for iqs in image_ques:\n",
    "            prompt=iqs\n",
    "            inputs = processor(images=img, text=prompt, return_tensors=\"pt\").to(device)\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=False,\n",
    "                num_beams=5,\n",
    "                max_length=256,\n",
    "                min_length=1,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.5,\n",
    "                length_penalty=1.0,\n",
    "                temperature=1,\n",
    "            )\n",
    "            generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "            # print(generated_text)\n",
    "            ansFile.write(f\"{prompt} Answer: {image_ans[count]}. Output: {generated_text}\\n\")\n",
    "            # ansFile.write(prompt +\" \" + image_ans[count] + \": \" + generated_text+\"\\n\")\n",
    "            count+=1\n",
    "    ansFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov  4 00:01:59 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    71W / 500W |  72199MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1054123      C   ...orch-gpu-2.0.1/bin/python    72196MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.0.1",
   "language": "python",
   "name": "pytorch-gpu-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
